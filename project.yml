title: "Spancat datasets"
description: |
  This project compiles various spancat datasets and their converters into the
  [spaCy format](https://spacy.io/api/data-formats).

vars:
  spans_key: "sc"
  gpu_id: 0
  ner_config: "ner_tok2vec"
  spancat_config: "spancat_tok2vec_ngram"

directories:
  - "assets"
  - "configs"
  - "corpus"
  - "corpus/spancat"
  - "corpus/ner"
  - "scripts"
  - "training"
  - "metrics"
  - "vectors"
  - "tables"

workflows:
  wnut17-ner:
    - "convert-wnut17-ents"
    - "train-wnut17-ner"
    - "evaluate-wnut17-ner"
  wnut17-spancat:
    - "convert-wnut17-spans"
    - "train-wnut17-spancat"
    - "evaluate-wnut17-spancat"
  wikineural-ner:
    - "clean-wikineural"
    - "convert-wikineural-ents"
    - "train-wikineural-ner"
    - "evaluate-wikineural-ner"
  wikineural-spancat:
    - "clean-wikineural"
    - "convert-wikineural-spans"
    - "train-wikineural-spancat"
    - "evaluate-wikineural-spancat"
  conll-ner:
    - "clean-conll"
    - "convert-conll-ents"
    - "train-conll-ner"
    - "evaluate-conll-ner"
  conll-spancat:
    - "clean-conll"
    - "convert-conll-spans"
    - "train-conll-spancat"
    - "evaluate-conll-spancat"

assets:
  # WNUT17
  - dest: "assets/wnut17-train.iob"
    description: "WNUT17 training dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/train/wnut17train.conll
  - dest: "assets/wnut17-dev.iob"
    description: "WNUT17 dev dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/dev/emerging.dev.conll
  - dest: "assets/wnut17-test.iob"
    description: "WNUT17 test dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/test/emerging.test.annotated
  ### Wikineural datasets ###
  # Wikineural (en)
  - dest: "assets/raw-en-wikineural-train.iob"
    description: "WikiNeural (en) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/en/train.conllu
  - dest: "assets/raw-en-wikineural-dev.iob"
    description: "WikiNeural (en) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/en/val.conllu
  - dest: "assets/raw-en-wikineural-test.iob"
    description: "WikiNeural (en) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/en/test.conllu
  # Wikineural (de)
  - dest: "assets/raw-de-wikineural-train.iob"
    description: "WikiNeural (de) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/de/train.conllu
  - dest: "assets/raw-de-wikineural-dev.iob"
    description: "WikiNeural (de) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/de/val.conllu
  - dest: "assets/raw-de-wikineural-test.iob"
    description: "WikiNeural (de) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/de/test.conllu
  # Wikineural (es)
  - dest: "assets/raw-es-wikineural-train.iob"
    description: "WikiNeural (es) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/es/train.conllu
  - dest: "assets/raw-es-wikineural-dev.iob"
    description: "WikiNeural (es) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/es/val.conllu
  - dest: "assets/raw-es-wikineural-test.iob"
    description: "WikiNeural (es) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/es/test.conllu
  # Wikineural (nl)
  - dest: "assets/raw-nl-wikineural-train.iob"
    description: "WikiNeural (nl) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/nl/train.conllu
  - dest: "assets/raw-nl-wikineural-dev.iob"
    description: "WikiNeural (nl) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/nl/val.conllu
  - dest: "assets/raw-nl-wikineural-test.iob"
    description: "WikiNeural (nl) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/wikineural/nl/test.conllu
  ### ConLL datasets ###
  #  CoNLL-2003 (en)
  - dest: "assets/raw-en-conll-train.iob"
    description: "CoNLL 2003 (en) training dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/train.conllu
  - dest: "assets/raw-en-conll-dev.iob"
    description: "CoNLL 2003 (en) dev dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/val.conllu
  - dest: "assets/raw-en-conll-test.iob"
    description: "CoNLL 2003 (en) test dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/test.conllu
  # CoNLL-2003 (de)
  - dest: "assets/raw-de-conll-train.iob"
    description: "CoNLL 2003 (de) training dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/train.conllu
  - dest: "assets/raw-de-conll-dev.iob"
    description: "CoNLL 2003 (de) dev dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/val.conllu
  - dest: "assets/raw-de-conll-test.iob"
    description: "CoNLL 2003 (de) test dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/test.conllu
  # CoNLL-2002 (es)
  - dest: "assets/raw-es-conll-train.iob"
    description: "CoNLL 2002 (es) training dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/train.conllu
  - dest: "assets/raw-es-conll-dev.iob"
    description: "CoNLL 2002 (es) dev dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/val.conllu
  - dest: "assets/raw-es-conll-test.iob"
    description: "CoNLL (es) test dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/test.conllu
  # CoNLL-2002 (nl)
  - dest: "assets/raw-nl-conll-train.iob"
    description: "CoNLL 2002 (nl) training dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/train.conllu
  - dest: "assets/raw-nl-conll-dev.iob"
    description: "CoNLL 2002 (nl) dev dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/val.conllu
  - dest: "assets/raw-nl-conll-test.iob"
    description: "CoNLL 202 (nl) test dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/test.conllu
  # Fastext vectors
  - dest: "assets/fasttext.en.gz"
    descrption: "English fastText vectors."
    url: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz
  - dest: "assets/fasttext.de.gz"
    description: "German fastText vectors."
    url: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz
  - dest: "assets/fasttext.es.gz"
    description: "Spanish fastText vectors."
    url: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz
  - dest: "assets/fasttext.nl.gz"
    description: "Dutch fastText vectors."
    url: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz

commands:
  - name: "install"
    help: "Install dependencies"
    script:
      - "python -m wandb login"
      - "python -m spacy download en_core_web_lg"
      - "python -m spacy download nl_core_news_lg"
      - "python -m spacy download de_core_news_lg"
      - "python -m spacy download es_core_news_lg"

  - name: "init-fasttext"
    help: "Initialize the FastText vectors"
    script:
      - "gzip -d assets/fasttext.en.gz"
      - "gzip -d assets/fasttext.de.gz"
      - "gzip -d assets/fasttext.es.gz"
      - "gzip -d assets/fasttext.nl.gz"
      - "python -m spacy init vectors en assets/fasttext.en vectors/fasttext-en"
      - "python -m spacy init vectors es assets/fasttext.es vectors/fasttext-es"
      - "python -m spacy init vectors de assets/fasttext.de vectors/fasttext-de"
      - "python -m spacy init vectors nl assets/fasttext.nl vectors/fasttext-nl"

  - name: "convert-wnut17-ents"
    help: "Convert WNUT17 dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-test.iob corpus/ner/
        --use-ents
    deps:
      - assets/wnut17-train.iob
      - assets/wnut17-dev.iob
      - assets/wnut17-test.iob
    outputs:
      - corpus/ner/wnut17-train.spacy
      - corpus/ner/wnut17-dev.spacy
      - corpus/ner/wnut17-test.spacy

  - name: "convert-wnut17-spans"
    help: "Convert WNUT17 dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
    deps:
      - assets/wnut17-train.iob
      - assets/wnut17-dev.iob
      - assets/wnut17-test.iob
    outputs:
      - corpus/spancat/wnut17-train.spacy
      - corpus/spancat/wnut17-dev.spacy
      - corpus/spancat/wnut17-test.spacy

  - name: "train-wnut17-ner"
    help: "Train an ner model for WNUT17"
    script:
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/wnut17/${vars.ner_config}/
        --paths.train corpus/ner/wnut17-train.spacy
        --paths.dev corpus/ner/wnut17-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/ner/wnut17-train.spacy
      - corpus/ner/wnut17-dev.spacy
    outputs:
      - training/wnut17/${vars.ner_config}/model-best

  - name: "train-wnut17-spancat"
    help: "Train a spancat model for WNUT17"
    script:
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/wnut17/${vars.spancat_config}/
        --paths.train corpus/spancat/wnut17-train.spacy
        --paths.dev corpus/spancat/wnut17-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/spancat/wnut17-train.spacy
      - corpus/spancat/wnut17-dev.spacy
    outputs:
      - training/wnut17/${vars.spancat_config}/model-best

  - name: "evaluate-wnut17-ner"
    help: "Evaluate the NER results for the WNUT17 dataset"
    script:
      - "mkdir -p metrics/wnut17/ner"
      - >-
        python -m spacy evaluate
        training/wnut17/${vars.ner_config}/model-best
        corpus/ner/wnut17-dev.spacy
        --output metrics/wnut17/${vars.ner_config}/scores.json
        --gpu-id ${vars.gpu_id}
    deps:
      - training/wnut17/${vars.ner_config}/model-best
      - corpus/ner/wnut17-dev.spacy
    outputs:
      - metrics/wnut17/${vars.ner_config}/scores.json

  - name: "evaluate-wnut17-spancat"
    help: "Evaluate the spancat results for the WNUT17 dataset"
    script:
      - "mkdir -p metrics/wnut17/spancat"
      - >-
        python -m spacy evaluate
        training/wnut17/${vars.spancat_config}/model-best
        corpus/spancat/wnut17-dev.spacy
        --output metrics/wnut17/${vars.spancat_config}/scores.json
        --gpu-id ${vars.gpu_id}
      - "mkdir -p metrics/wnut17/ner"
    deps:
      - training/wnut17/${vars.spancat_config}/model-best
      - corpus/spancat/wnut17-dev.spacy
    outputs:
      - metrics/wnut17/${vars.spancat_config}/scores.json

  - name: "clean-wikineural"
    help: "Remove unnecessary indices from wikineural data"
    script:
      # Clean Wikineural datasets
      - python -m scripts.preprocess_wikineural assets/raw-de-wikineural-train.iob assets/de-wikineural-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-de-wikineural-dev.iob assets/de-wikineural-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-de-wikineural-test.iob assets/de-wikineural-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-wikineural-train.iob assets/en-wikineural-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-wikineural-dev.iob assets/en-wikineural-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-wikineural-test.iob assets/en-wikineural-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-wikineural-train.iob assets/es-wikineural-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-wikineural-dev.iob assets/es-wikineural-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-wikineural-test.iob assets/es-wikineural-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-wikineural-train.iob assets/nl-wikineural-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-wikineural-dev.iob assets/nl-wikineural-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-wikineural-test.iob assets/nl-wikineural-test.iob
    deps:
      # Wikineural datasets
      - assets/raw-de-wikineural-train.iob
      - assets/raw-de-wikineural-dev.iob
      - assets/raw-en-wikineural-train.iob
      - assets/raw-en-wikineural-dev.iob
      - assets/raw-en-wikineural-test.iob
      - assets/raw-es-wikineural-train.iob
      - assets/raw-es-wikineural-dev.iob
      - assets/raw-es-wikineural-test.iob
      - assets/raw-nl-wikineural-train.iob
      - assets/raw-nl-wikineural-dev.iob
      - assets/raw-nl-wikineural-test.iob
    outputs:
      # Cleaned Wikineural datasets
      - assets/de-wikineural-train.iob
      - assets/de-wikineural-dev.iob
      - assets/en-wikineural-train.iob
      - assets/en-wikineural-dev.iob
      - assets/en-wikineural-test.iob
      - assets/es-wikineural-train.iob
      - assets/es-wikineural-dev.iob
      - assets/es-wikineural-test.iob
      - assets/nl-wikineural-train.iob
      - assets/nl-wikineural-dev.iob
      - assets/nl-wikineural-test.iob

  - name: "convert-wikineural-spans"
    help: "Convert WikiNeural dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # - python -m scripts.preprocess_wikineural assets/de-wikineural-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-wikineural-test.iob corpus/spancat/
      #   --spans-key ${vars.spans_key}
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - "assets/de-wikineural-train.iob"
      - "assets/de-wikineural-dev.iob"
      # - "assets/de-wikineural-test.iob"
      - "assets/en-wikineural-train.iob"
      - "assets/en-wikineural-dev.iob"
      - "assets/en-wikineural-test.iob"
      - "assets/es-wikineural-train.iob"
      - "assets/es-wikineural-dev.iob"
      - "assets/es-wikineural-test.iob"
      - "assets/nl-wikineural-train.iob"
      - "assets/nl-wikineural-dev.iob"
      - "assets/nl-wikineural-test.iob"
    outputs:
      - "corpus/spancat/de-wikineural-train.spacy"
      - "corpus/spancat/de-wikineural-dev.spacy"
      # - "corpus/spancat/de-wikineural-test.spacy"
      - "corpus/spancat/en-wikineural-train.spacy"
      - "corpus/spancat/en-wikineural-dev.spacy"
      - "corpus/spancat/en-wikineural-test.spacy"
      - "corpus/spancat/es-wikineural-train.spacy"
      - "corpus/spancat/es-wikineural-dev.spacy"
      - "corpus/spancat/es-wikineural-test.spacy"
      - "corpus/spancat/nl-wikineural-train.spacy"
      - "corpus/spancat/nl-wikineural-dev.spacy"
      - "corpus/spancat/nl-wikineural-test.spacy"

  - name: "convert-wikineural-ents"
    help: "Convert WikiNeural dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-dev.iob corpus/ner/
        --use-ents
      # - python -m scripts.preprocess_wikineural assets/de-wikineural-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-wikineural-test.iob corpus/spancat/
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-test.iob corpus/ner/
        --use-ents
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-test.iob corpus/ner/
        --use-ents
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-test.iob corpus/ner/
        --use-ents
    deps:
      - "assets/de-wikineural-train.iob"
      - "assets/de-wikineural-dev.iob"
      # - "assets/de-wikineural-test.iob"
      - "assets/en-wikineural-train.iob"
      - "assets/en-wikineural-dev.iob"
      - "assets/en-wikineural-test.iob"
      - "assets/es-wikineural-train.iob"
      - "assets/es-wikineural-dev.iob"
      - "assets/es-wikineural-test.iob"
      - "assets/nl-wikineural-train.iob"
      - "assets/nl-wikineural-dev.iob"
      - "assets/nl-wikineural-test.iob"
    outputs:
      - "corpus/ner/de-wikineural-train.spacy"
      - "corpus/ner/de-wikineural-dev.spacy"
      # - "corpus/spancat/de-wikineural-test.spacy"
      - "corpus/ner/en-wikineural-train.spacy"
      - "corpus/ner/en-wikineural-dev.spacy"
      - "corpus/ner/en-wikineural-test.spacy"
      - "corpus/ner/es-wikineural-train.spacy"
      - "corpus/ner/es-wikineural-dev.spacy"
      - "corpus/ner/es-wikineural-test.spacy"
      - "corpus/ner/nl-wikineural-train.spacy"
      - "corpus/ner/nl-wikineural-dev.spacy"
      - "corpus/ner/nl-wikineural-test.spacy"

  - name: "make-wikineural-tables"
    help: "Pre-compute token-to-id tables from the Wikineural training sets."
    script:
      - >-
        python scripts/token_map.py
        corpus/ner/de-wikineural-train.spacy
        tables/wikineural.de.table
        --language de

      - >-
        python scripts/token_map.py
        corpus/ner/en-wikineural-train.spacy
        tables/wikineural.en.table
        --language en

      - >-
        python scripts/token_map.py
        corpus/ner/es-wikineural-train.spacy
        tables/wikineural.es.table
        --language es

      - >-
        python scripts/token_map.py
        corpus/ner/nl-wikineural-train.spacy
        tables/wikineural.nl.table
        --language nl

  - name: "train-wikineural-spancat"
    help: "Train a spancat model for Wikineural datasets"
    script:
      # Train de dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/de-wikineural/${vars.spancat_config}/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/spancat/de-wikineural-train.spacy
        --paths.dev corpus/spancat/de-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train en dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/en-wikineural/${vars.spancat_config}/
        --paths.train corpus/spancat/en-wikineural-train.spacy
        --paths.dev corpus/spancat/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train es dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/es-wikineural/${vars.spancat_config}/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/spancat/es-wikineural-train.spacy
        --paths.dev corpus/spancat/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train nl dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/nl-wikineural/${vars.spancat_config}/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/spancat/nl-wikineural-train.spacy
        --paths.dev corpus/spancat/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
    outputs:
      - training/de-wikineural/${vars.spancat_config}/model-best
      - training/en-wikineural/${vars.spancat_config}/model-best
      - training/es-wikineural/${vars.spancat_config}/model-best
      - training/nl-wikineural/${vars.spancat_config}/model-best

  - name: "train-wikineural-ner"
    help: "Train an ner model for Wikineural datasets"
    script:
      # Train de dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/de-wikineural/${vars.ner_config}/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/ner/de-wikineural-train.spacy
        --paths.dev corpus/ner/de-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --paths.charlstm training/de-wikineural/ner_charlstm/model-best
        --paths.tokenlstm training/de-wikineural/ner_tokenlstm/model-best
        --code scripts/custom_components.py
      # Train en dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/en-wikineural/${vars.ner_config}/
        --paths.train corpus/ner/en-wikineural-train.spacy
        --paths.dev corpus/ner/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train es dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/es-wikineural/${vars.ner_config}/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/ner/es-wikineural-train.spacy
        --paths.dev corpus/ner/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train nl dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/nl-wikineural/${vars.ner_config}/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/ner/nl-wikineural-train.spacy
        --paths.dev corpus/ner/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
    outputs:
      - training/de-wikineural/${vars.ner_config}/model-best
      - training/en-wikineural/${vars.ner_config}/model-best
      - training/es-wikineural/${vars.ner_config}/model-best
      - training/nl-wikineural/${vars.ner_config}/model-best

  - name: "evaluate-wikineural-ner"
    help: "Evaluate the ner results for the Wikineural datasets"
    script:
      # FIXME: There seems to be something wrong in the ConLL
      # dataset for the 'de' test set, so I can't evaluate it yet
      # Evaluate en dataset
      - "mkdir -p metrics/en-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/en-wikineural/${vars.ner_config}/model-best
        corpus/ner/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/en-wikineural/${vars.ner_config}/scores.json
      # Evaluate es dataset
      - "mkdir -p metrics/es-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/es-wikineural/${vars.ner_config}/model-best
        corpus/ner/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/es-wikineural/${vars.ner_config}/scores.json
      # Evaluate nl dataset
      - "mkdir -p metrics/nl-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/nl-wikineural/${vars.ner_config}/model-best
        corpus/ner/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/nl-wikineural/${vars.ner_config}/scores.json
    deps:
      - training/en-wikineural/${vars.ner_config}/model-best
      - training/es-wikineural/${vars.ner_config}/model-best
      - training/nl-wikineural/${vars.ner_config}/model-best
      - corpus/ner/en-wikineural-dev.spacy
      - corpus/ner/es-wikineural-dev.spacy
      - corpus/ner/nl-wikineural-dev.spacy
    outputs:
      - metrics/en-wikineural/${vars.ner_config}/scores.json
      - metrics/es-wikineural/${vars.ner_config}/scores.json
      - metrics/nl-wikineural/${vars.ner_config}/scores.json

  - name: "evaluate-wikineural-spancat"
    help: "Evaluate the spancat results for the Wikineural datasets"
    script:
      # FIXME: There seems to be something wrong in the ConLL
      # dataset for the 'de' test set, so I can't evaluate it yet
      # Evaluate en dataset
      - "mkdir -p metrics/en-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/en-wikineural/${vars.spancat_config}/model-best
        corpus/spancat/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/en-wikineural/${vars.spancat_config}/scores.json
      # Evaluate es dataset
      - "mkdir -p metrics/es-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/es-wikineural/${vars.spancat_config}/model-best
        corpus/spancat/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/es-wikineural/${vars.spancat_config}/scores.json
      # Evaluate nl dataset
      - "mkdir -p metrics/nl-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/nl-wikineural/${vars.spancat_config}/model-best
        corpus/spancat/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/nl-wikineural/${vars.spancat_config}/scores.json
    deps:
      - training/en-wikineural/${vars.spancat_config}/model-best
      - training/es-wikineural/${vars.spancat_config}/model-best
      - training/nl-wikineural/${vars.spancat_config}/model-best
      - corpus/spancat/en-wikineural-dev.spacy
      - corpus/spancat/es-wikineural-dev.spacy
      - corpus/spancat/nl-wikineural-dev.spacy
    outputs:
      - metrics/en-wikineural/${vars.spancat_config}/scores.json
      - metrics/es-wikineural/${vars.spancat_config}/scores.json
      - metrics/nl-wikineural/${vars.spancat_config}/scores.json

  - name: "clean-conll"
    help: "Remove unnecessary indices from ConLL data"
    script:
      # Clean CoNLL datasets
      - python -m scripts.preprocess_wikineural assets/raw-de-conll-train.iob assets/de-conll-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-de-conll-dev.iob assets/de-conll-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-de-conll-test.iob assets/de-conll-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-conll-train.iob assets/en-conll-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-conll-dev.iob assets/en-conll-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-en-conll-test.iob assets/en-conll-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-conll-train.iob assets/es-conll-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-conll-dev.iob assets/es-conll-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-es-conll-test.iob assets/es-conll-test.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-conll-train.iob assets/nl-conll-train.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-conll-dev.iob assets/nl-conll-dev.iob
      - python -m scripts.preprocess_wikineural assets/raw-nl-conll-test.iob assets/nl-conll-test.iob
    deps:
      # CoNLL datasets
      - assets/raw-de-conll-train.iob
      - assets/raw-de-conll-dev.iob
      - assets/raw-en-conll-train.iob
      - assets/raw-en-conll-dev.iob
      - assets/raw-en-conll-test.iob
      - assets/raw-es-conll-train.iob
      - assets/raw-es-conll-dev.iob
      - assets/raw-es-conll-test.iob
      - assets/raw-nl-conll-train.iob
      - assets/raw-nl-conll-dev.iob
      - assets/raw-nl-conll-test.iob
    outputs:
      # Cleaned CoNLL datasets
      - assets/de-conll-train.iob
      - assets/de-conll-dev.iob
      - assets/en-conll-train.iob
      - assets/en-conll-dev.iob
      - assets/en-conll-test.iob
      - assets/es-conll-train.iob
      - assets/es-conll-dev.iob
      - assets/es-conll-test.iob
      - assets/nl-conll-train.iob
      - assets/nl-conll-dev.iob
      - assets/nl-conll-test.iob

  - name: "convert-conll-spans"
    help: "Convert CoNLL dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-conll-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/de-conll-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # - python -m scripts.preprocess_conll assets/de-conll-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-conll-test.iob corpus/spancat/
      #   --spans-key ${vars.spans_key}
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - "assets/de-conll-train.iob"
      - "assets/de-conll-dev.iob"
      # - "assets/de-conll-test.iob"
      - "assets/en-conll-train.iob"
      - "assets/en-conll-dev.iob"
      - "assets/en-conll-test.iob"
      - "assets/es-conll-train.iob"
      - "assets/es-conll-dev.iob"
      - "assets/es-conll-test.iob"
      - "assets/nl-conll-train.iob"
      - "assets/nl-conll-dev.iob"
      - "assets/nl-conll-test.iob"
    outputs:
      - "corpus/spancat/de-conll-train.spacy"
      - "corpus/spancat/de-conll-dev.spacy"
      # - "corpus/spancat/de-conll-test.spacy"
      - "corpus/spancat/en-conll-train.spacy"
      - "corpus/spancat/en-conll-dev.spacy"
      - "corpus/spancat/en-conll-test.spacy"
      - "corpus/spancat/es-conll-train.spacy"
      - "corpus/spancat/es-conll-dev.spacy"
      - "corpus/spancat/es-conll-test.spacy"
      - "corpus/spancat/nl-conll-train.spacy"
      - "corpus/spancat/nl-conll-dev.spacy"
      - "corpus/spancat/nl-conll-test.spacy"

  - name: "convert-conll-ents"
    help: "Convert CoNLL dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-conll-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/de-conll-dev.iob corpus/ner/
        --use-ents
      # - python -m scripts.preprocess_conll assets/de-conll-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-conll-test.iob corpus/spancat/
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-conll-test.iob corpus/ner/
        --use-ents
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-conll-test.iob corpus/ner/
        --use-ents
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-conll-test.iob corpus/ner/
        --use-ents
    deps:
      - "assets/de-conll-train.iob"
      - "assets/de-conll-dev.iob"
      # - "assets/de-conll-test.iob"
      - "assets/en-conll-train.iob"
      - "assets/en-conll-dev.iob"
      - "assets/en-conll-test.iob"
      - "assets/es-conll-train.iob"
      - "assets/es-conll-dev.iob"
      - "assets/es-conll-test.iob"
      - "assets/nl-conll-train.iob"
      - "assets/nl-conll-dev.iob"
      - "assets/nl-conll-test.iob"
    outputs:
      - "corpus/ner/de-conll-train.spacy"
      - "corpus/ner/de-conll-dev.spacy"
      # - "corpus/spancat/de-conll-test.spacy"
      - "corpus/ner/en-conll-train.spacy"
      - "corpus/ner/en-conll-dev.spacy"
      - "corpus/ner/en-conll-test.spacy"
      - "corpus/ner/es-conll-train.spacy"
      - "corpus/ner/es-conll-dev.spacy"
      - "corpus/ner/es-conll-test.spacy"
      - "corpus/ner/nl-conll-train.spacy"
      - "corpus/ner/nl-conll-dev.spacy"
      - "corpus/ner/nl-conll-test.spacy"

  - name: "make-conll-tables"
    help: "Pre-compute token-to-id tables from the Wikineural training sets."
    script:
      - >-
        python scripts/token_map.py
        corpus/ner/de-conll-train.spacy
        tables/conll.de.table
        --language de

      - >-
        python scripts/token_map.py
        corpus/ner/en-conll-train.spacy
        tables/conll.en.table
        --language en

      - >-
        python scripts/token_map.py
        corpus/ner/es-conll-train.spacy
        tables/conll.es.table
        --language es

      - >-
        python scripts/token_map.py
        corpus/ner/nl-conll-train.spacy
        tables/conll.nl.table
        --language nl

  - name: "train-conll-spancat"
    help: "Train a spancat model for Wikineural datasets"
    script:
      # Train de dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/de-conll/${vars.spancat_config}/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/spancat/de-conll-train.spacy
        --paths.dev corpus/spancat/de-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train en dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/en-conll/${vars.spancat_config}/
        --paths.train corpus/spancat/en-conll-train.spacy
        --paths.dev corpus/spancat/en-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train es dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/es-conll/${vars.spancat_config}/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/spancat/es-conll-train.spacy
        --paths.dev corpus/spancat/es-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train nl dataset
      - >-
        python -m spacy train
        configs/spancat/${vars.spancat_config}.cfg
        --output training/nl-conll/${vars.spancat_config}/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/spancat/nl-conll-train.spacy
        --paths.dev corpus/spancat/nl-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
    outputs:
      - training/de-conll/${vars.spancat_config}/model-best
      - training/en-conll/${vars.spancat_config}/model-best
      - training/es-conll/${vars.spancat_config}/model-best
      - training/nl-conll/${vars.spancat_config}/model-best

  - name: "train-conll-ner"
    help: "Train an ner model for Wikineural datasets"
    script:
      # Train de dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/de-conll/${vars.ner_config}/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/ner/de-conll-train.spacy
        --paths.dev corpus/ner/de-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --paths.charlstm training/de-conll/ner_charlstm/model-best
        --paths.tokenlstm training/de-conll/ner_tokenlstm/model-best
        --code scripts/custom_components.py
      # Train en dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/en-conll/${vars.ner_config}/
        --paths.train corpus/ner/en-conll-train.spacy
        --paths.dev corpus/ner/en-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train es dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/es-conll/${vars.ner_config}/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/ner/es-conll-train.spacy
        --paths.dev corpus/ner/es-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train nl dataset
      - >-
        python -m spacy train
        configs/ner/${vars.ner_config}.cfg
        --output training/nl-conll/${vars.ner_config}/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/ner/nl-conll-train.spacy
        --paths.dev corpus/ner/nl-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
    outputs:
      - training/de-conll/${vars.ner_config}/model-best
      - training/en-conll/${vars.ner_config}/model-best
      - training/es-conll/${vars.ner_config}/model-best
      - training/nl-conll/${vars.ner_config}/model-best

  - name: "evaluate-conll-ner"
    help: "Evaluate the ner results for the CoNLL datasets"
    script:
      # FIXME: There seems to be something wrong in the ConLL
      # dataset for the 'de' test set, so I can't evaluate it yet
      # Evaluate en dataset
      - "mkdir -p metrics/en-conll/ner"
      - >-
        python -m spacy evaluate
        training/en-conll/${vars.ner_config}/model-best
        corpus/ner/en-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/en-conll/${vars.ner_config}/scores.json
      # Evaluate es dataset
      - "mkdir -p metrics/es-conll/ner"
      - >-
        python -m spacy evaluate
        training/es-conll/${vars.ner_config}/model-best
        corpus/ner/es-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/es-conll/${vars.ner_config}/scores.json
      # Evaluate nl dataset
      - "mkdir -p metrics/nl-conll/ner"
      - >-
        python -m spacy evaluate
        training/nl-conll/${vars.ner_config}/model-best
        corpus/ner/nl-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/nl-conll/${vars.ner_config}/scores.json
    deps:
      - training/en-conll/${vars.ner_config}/model-best
      - training/es-conll/${vars.ner_config}/model-best
      - training/nl-conll/${vars.ner_config}/model-best
      - corpus/ner/en-conll-dev.spacy
      - corpus/ner/es-conll-dev.spacy
      - corpus/ner/nl-conll-dev.spacy
    outputs:
      - metrics/en-conll/${vars.ner_config}/scores.json
      - metrics/es-conll/${vars.ner_config}/scores.json
      - metrics/nl-conll/${vars.ner_config}/scores.json

  - name: "evaluate-conll-spancat"
    help: "Evaluate the spancat results for the Wikineural datasets"
    script:
      # FIXME: There seems to be something wrong in the ConLL
      # dataset for the 'de' test set, so I can't evaluate it yet
      # Evaluate en dataset
      - "mkdir -p metrics/en-conll/spancat"
      - >-
        python -m spacy evaluate
        training/en-conll/${vars.spancat_config}/model-best
        corpus/spancat/en-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/en-conll/${vars.spancat_config}/scores.json
      # Evaluate es dataset
      - "mkdir -p metrics/es-conll/spancat"
      - >-
        python -m spacy evaluate
        training/es-conll/${vars.spancat_config}/model-best
        corpus/spancat/es-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/es-conll/${vars.spancat_config}/scores.json
      # Evaluate nl dataset
      - "mkdir -p metrics/nl-conll/spancat"
      - >-
        python -m spacy evaluate
        training/nl-conll/${vars.spancat_config}/model-best
        corpus/spancat/nl-conll-dev.spacy
        --gpu-id ${vars.gpu_id}
        --output metrics/nl-conll/${vars.spancat_config}/scores.json
    deps:
      - training/en-conll/${vars.spancat_config}/model-best
      - training/es-conll/${vars.spancat_config}/model-best
      - training/nl-conll/${vars.spancat_config}/model-best
      - corpus/spancat/en-conll-dev.spacy
      - corpus/spancat/es-conll-dev.spacy
      - corpus/spancat/nl-conll-dev.spacy
    outputs:
      - metrics/en-conll/${vars.spancat_config}/scores.json
      - metrics/es-conll/${vars.spancat_config}/scores.json
      - metrics/nl-conll/${vars.spancat_config}/scores.json

  - name: "clean"
    help: "Remove intermediary files"
    script:
      - "rm -rf assets/*"
      - "rm -rf corpus/*"
      - "rm -rf training/*"
      - "rm -rf metrics/*"
