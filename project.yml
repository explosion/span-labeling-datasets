title: "Spancat datasets"
description: |
  This project compiles various spancat datasets and their converters into the
  [spaCy format](https://spacy.io/api/data-formats).

vars:
  spans_key: "sc"
  gpu_id: 0

directories:
  - "assets"
  - "configs"
  - "corpus"
  - "corpus/spancat"
  - "corpus/ner"
  - "scripts"
  - "training"
  - "metrics"

workflows:
  anem:
    - "convert-anem"
    - "train-anem"
    - "evaluate-anem"
  sec-filings:
    - "convert-sec_filings"
    - "train-sec_filings"
    - "evaluate-sec_filings"
  wnut17:
    - "convert-wnut17"
    - "train-wnut17"
    - "evaluate-wnut17"
  wikineural:
    - "clean-wikineural"
    - "convert-wikineural-ents"
    - "convert-wikineural-spans"
    - "train-wikineural"
    - "evaluate-wikineural"
  toxic-spans:
    - "convert-toxic-spans"
    - "train-toxic-spans"
    - "evaluate-toxic-spans"
  pdnc:
    - "convert-pdnc"
    - "train-pdnc"
    - "evaluate-pdnc"

assets:
  - dest: "assets/sec_filings-train.iob"
    description: "SEC filings training dataset from Alvarado et al. (ALTA 2015)"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/SEC-filings/CONLL-format/data/train/FIN5.txt
  - dest: "assets/sec_filings-test.iob"
    description: "SEC filings test dataset from Alvarado et al. (ALTA 2015)"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/SEC-filings/CONLL-format/data/test/FIN3.txt
  - dest: "assets/wnut17-train.iob"
    description: "WNUT17 training dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/train/wnut17train.conll
  - dest: "assets/wnut17-dev.iob"
    description: "WNUT17 dev dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/dev/emerging.dev.conll
  - dest: "assets/wnut17-test.iob"
    description: "WNUT17 test dataset for Emerging and Rare Entities Task from Derczynski et al., 2017"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/test/emerging.test.annotated
  - dest: "assets/btc-general.iob"
    description: "Broad Twitter Corpus (BTC) containing UK general tweets from Derczynski et al., 2016"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/BTC/CONLL-format/data/a.conll
  - dest: "assets/anem-train.iob"
    description: "Anatomical Entity Mention (AnEM) training corpus containing abstracts and full-text biomedical papers from Ohta et al. (ACL 2012)"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/AnEM/CONLL-format/data/AnEM.train
  - dest: "assets/anem-test.iob"
    description: "Anatomical Entity Mention (AnEM) test corpus containing abstracts and full-text biomedical papers from Ohta et al. (ACL 2012)"
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/AnEM/CONLL-format/data/AnEM.test
  - dest: "assets/wikigold.iob"
    description: "Wikigold dataset containing a manually annotated collection of Wikipedia text by Balasuriya et al. (ACL 2009)."
    url: https://github.com/juand-r/entity-recognition-datasets/blob/master/data/wikigold/CONLL-format/data/wikigold.conll.txt
  - dest: "assets/en-wikineural-train.iob"
    description: "WikiNeural (en) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/train.conllu
  - dest: "assets/en-wikineural-dev.iob"
    description: "WikiNeural (en) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/val.conllu
  - dest: "assets/en-wikineural-test.iob"
    description: "WikiNeural (en) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/test.conllu
  - dest: "assets/de-wikineural-train.iob"
    description: "WikiNeural (de) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/train.conllu
  - dest: "assets/de-wikineural-dev.iob"
    description: "WikiNeural (de) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/val.conllu
  - dest: "assets/de-wikineural-test.iob"
    description: "WikiNeural (de) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/de/test.conllu
  - dest: "assets/es-wikineural-train.iob"
    description: "WikiNeural (es) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/train.conllu
  - dest: "assets/es-wikineural-dev.iob"
    description: "WikiNeural (es) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/val.conllu
  - dest: "assets/es-wikineural-test.iob"
    description: "WikiNeural (es) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/es/test.conllu
  - dest: "assets/nl-wikineural-train.iob"
    description: "WikiNeural (nl) training dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/train.conllu
  - dest: "assets/nl-wikineural-dev.iob"
    description: "WikiNeural (nl) dev dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/val.conllu
  - dest: "assets/nl-wikineural-test.iob"
    description: "WikiNeural (nl) test dataset from Tedeschi et al. (EMNLP 2021)"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/nl/test.conllu
  - dest: "assets/toxic_spans.csv"
    description: "Toxic Spans dataset from Pavlopoulos et al. (ACL 2022)"
    url: https://github.com/ipavlopoulos/toxic_spans/blob/master/ACL2022/data/toxic_spans.csv
  - dest: "assets/pdnc/"
    description: "Raw text data for the PDNC dataset"
    git:
      repo: "https://github.com/Priya22/pdnc-lrec2022"
      branch: "master"
      path: "data"
  - dest: "assets/pdnc/list_of_novels.txt"
    description: "Novel list for the PDNC dataset as reference"
    url: https://github.com/ljvmiranda921/pdnc-lrec2022/blob/patch-1/ListOfNovels.txt

commands:
  - name: "install"
    help: "Install dependencies"
    script:
      - "python -m spacy download en_core_web_lg"

  - name: "convert-pdnc"
    help: "Convert the PDNC dataset into the spaCy format"
    script:
      - >-
        python -m scripts.pdnc 
        assets/pdnc corpus/spancat/
        --spans-key ${vars.spans_key}
        --shuffle
        --seed 42
      - >-
        python -m scripts.pdnc 
        assets/pdnc corpus/ner/
        --use-ents
        --shuffle
        --seed 42
    deps:
      - assets/pdnc
    outputs:
      - corpus/spancat/pdnc-train.spacy
      - corpus/spancat/pdnc-dev.spacy
      - corpus/spancat/pdnc-test.spacy

  - name: "train-pdnc"
    help: "Train ner and spancat models for the PDNC dataset"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/pdnc/ner/
        --paths.train corpus/ner/pdnc-train.spacy
        --paths.dev corpus/ner/pdnc-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/pdnc/spancat/
        --paths.train corpus/spancat/pdnc-train.spacy
        --paths.dev corpus/spancat/pdnc-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/spancat/pdnc-train.spacy
      - corpus/spancat/pdnc-dev.spacy
      - corpus/ner/pdnc-train.spacy
      - corpus/ner/pdnc-dev.spacy
    outputs:
      - training/pdnc/spancat/model-best
      - training/pdnc/ner/model-best

  - name: "evaluate-pdnc"
    help: "Evaluate models for the PDNC dataset"
    script:
      - "mkdir -p metrics/pdnc/ner"
      - >-
        python -m spacy evaluate
        training/pdnc/ner/model-best
        corpus/ner/pdnc-test.spacy
        --output metrics/pdnc/ner/scores.json
      - "mkdir -p metrics/pdnc/spancat"
      - >-
        python -m spacy evaluate
        training/pdnc/spancat/model-best
        corpus/spancat/pdnc-test.spacy
        --output metrics/pdnc/spancat/scores.json
    deps:
      - training/pdnc/spancat/model-best
      - corpus/spancat/pdnc-test.spacy
      - training/pdnc/ner/model-best
      - corpus/ner/pdnc-test.spacy
    outputs:
      - metrics/pdnc/ner/scores.json
      - metrics/pdnc/spancat/scores.json

  - name: "convert-toxic-spans"
    help: "Convert Toxic Spans dataset into the spaCy format"
    script:
      - >-
        python -m scripts.toxic_spans 
        assets/toxic_spans.csv corpus/spancat/
        --spans-key ${vars.spans_key}
        --shuffle
        --seed 42
      - >-
        python -m scripts.toxic_spans 
        assets/toxic_spans.csv corpus/ner/
        --use-ents
        --shuffle
        --seed 42
    deps:
      - assets/toxic_spans.csv
    outputs:
      - corpus/spancat/toxic-spans-train.spacy
      - corpus/spancat/toxic-spans-dev.spacy
      - corpus/spancat/toxic-spans-test.spacy
      - corpus/ner/toxic-spans-train.spacy
      - corpus/ner/toxic-spans-dev.spacy
      - corpus/ner/toxic-spans-test.spacy

  - name: "train-toxic-spans"
    help: "Train ner and spancat models for the Toxic Spans dataset"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/toxic-spans/ner/
        --paths.train corpus/ner/toxic-spans-train.spacy
        --paths.dev corpus/ner/toxic-spans-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/toxic-spans/spancat/
        --paths.train corpus/spancat/toxic-spans-train.spacy
        --paths.dev corpus/spancat/toxic-spans-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/spancat/toxic-spans-train.spacy
      - corpus/spancat/toxic-spans-dev.spacy
      - corpus/ner/toxic-spans-train.spacy
      - corpus/ner/toxic-spans-dev.spacy
    outputs:
      - training/toxic-spans/spancat/model-best
      - training/toxic-spans/ner/model-best

  - name: "evaluate-toxic-spans"
    help: "Evaluate models for the Toxic Spans dataset"
    script:
      - "mkdir -p metrics/toxic-spans/ner"
      - >-
        python -m spacy evaluate
        training/toxic-spans/ner/model-best
        corpus/ner/toxic-spans-test.spacy
        --output metrics/toxic-spans/ner/scores.json
      - "mkdir -p metrics/toxic-spans/spancat"
      - >-
        python -m spacy evaluate
        training/toxic-spans/spancat/model-best
        corpus/spancat/toxic-spans-test.spacy
        --output metrics/toxic-spans/spancat/scores.json
    deps:
      - training/toxic-spans/spancat/model-best
      - corpus/spancat/toxic-spans-test.spacy
      - training/toxic-spans/ner/model-best
      - corpus/ner/toxic-spans-test.spacy
    outputs:
      - metrics/toxic-spans/ner/scores.json
      - metrics/toxic-spans/spancat/scores.json

  - name: "convert-sec_filings"
    help: "Convert SEC filings dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/sec_filings-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
        --train-size 0.8
        --seed 42
        --shuffle
      - >-
        python -m scripts.convert_to_spans
        assets/sec_filings-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # NER conversion doc.ents
      - >-
        python -m scripts.convert_to_spans
        assets/sec_filings-train.iob corpus/ner/
        --use-ents
        --converter auto
        --train-size 0.8
        --seed 42
        --shuffle
      - >-
        python -m scripts.convert_to_spans
        assets/sec_filings-test.iob corpus/ner/
        --use-ents
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - assets/sec_filings-train.iob
      - assets/sec_filings-test.iob
    outputs:
      - corpus/spancat/sec_filings-train.spacy
      - corpus/spancat/sec_filings-train-dev.spacy
      - corpus/spancat/sec_filings-test.spacy
      - corpus/ner/sec_filings-train-dev.spacy
      - corpus/ner/sec_filings-test.spacy

  - name: "train-sec_filings"
    help: "Train an NER and Spancat model for SEC Filings dataset"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/sec_filings/ner/
        --paths.train corpus/ner/sec_filings-train.spacy
        --paths.dev corpus/ner/sec_filings-train-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/sec_filings/spancat/
        --paths.train corpus/spancat/sec_filings-train.spacy
        --paths.dev corpus/spancat/sec_filings-train-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/spancat/sec_filings-train.spacy
      - corpus/spancat/sec_filings-train-dev.spacy
      - corpus/ner/sec_filings-train.spacy
      - corpus/ner/sec_filings-train-dev.spacy
    outputs:
      - training/sec_filings/ner/model-best
      - training/sec_filings/spancat/model-best

  - name: "evaluate-sec_filings"
    help: "Evaluate the results for the SEC Filings dataset"
    script:
      - "mkdir -p metrics/sec_filings/spancat"
      - >-
        python -m spacy evaluate
        training/sec_filings/spancat/model-best
        corpus/spancat/sec_filings-test.spacy
        --output metrics/sec_filings/spancat/scores.json
      - "mkdir -p metrics/sec_filings/ner"
      - >-
        python -m spacy evaluate
        training/sec_filings/ner/model-best
        corpus/ner/sec_filings-test.spacy
        --output metrics/sec_filings/ner/scores.json
    deps:
      - training/sec_filings/ner/model-best
      - training/sec_filings/spancat/model-best
      - corpus/ner/sec_filings-test.spacy
      - corpus/spancat/sec_filings-test.spacy
    outputs:
      - metrics/sec_filings/ner/scores.json
      - metrics/sec_filings/spancat/scores.json

  - name: "convert-wnut17"
    help: "Convert WNUT17 dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
      # NER
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/wnut17-test.iob corpus/ner/
        --use-ents
    deps:
      - assets/wnut17-train.iob
      - assets/wnut17-dev.iob
      - assets/wnut17-test.iob
    outputs:
      - corpus/ner/wnut17-train.spacy
      - corpus/ner/wnut17-dev.spacy
      - corpus/ner/wnut17-test.spacy
      - corpus/spancat/wnut17-train.spacy
      - corpus/spancat/wnut17-dev.spacy
      - corpus/spancat/wnut17-test.spacy

  - name: "train-wnut17"
    help: "Train an ner and spancat model for WNUT17"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/wnut17/ner/
        --paths.train corpus/ner/wnut17-train.spacy
        --paths.dev corpus/ner/wnut17-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/wnut17/spancat/
        --paths.train corpus/spancat/wnut17-train.spacy
        --paths.dev corpus/spancat/wnut17-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/spancat/wnut17-train.spacy
      - corpus/spancat/wnut17-dev.spacy
      - corpus/ner/wnut17-train.spacy
      - corpus/ner/wnut17-dev.spacy
    outputs:
      - training/wnut17/ner/model-best
      - training/wnut17/spancat/model-best

  - name: "evaluate-wnut17"
    help: "Evaluate the results for the WNUT17 dataset"
    script:
      - "mkdir -p metrics/wnut17/spancat"
      - >-
        python -m spacy evaluate
        training/wnut17/spancat/model-best
        corpus/spancat/wnut17-test.spacy
        --output metrics/wnut17/spancat/scores.json
      - "mkdir -p metrics/wnut17/ner"
      - >-
        python -m spacy evaluate
        training/wnut17/ner/model-best
        corpus/ner/wnut17-test.spacy
        --output metrics/wnut17/ner/scores.json
    deps:
      - training/wnut17/ner/model-best
      - training/wnut17/spancat/model-best
      - corpus/ner/wnut17-test.spacy
      - corpus/spancat/wnut17-test.spacy
    outputs:
      - metrics/wnut17/ner/scores.json
      - metrics/wnut17/spancat/scores.json

  - name: "convert-btc"
    help: "Convert BTC dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/btc-general.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - assets/btc-general.iob
    outputs:
      - corpus/spancat/btc-general.spacy

  - name: "convert-anem"
    help: "Convert AneM dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/anem-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
        --train-size 0.8
        --seed 42
        --shuffle
      - >-
        python -m scripts.convert_to_spans
        assets/anem-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # NER conversion doc.ents
      - >-
        python -m scripts.convert_to_spans
        assets/anem-train.iob corpus/ner/
        --use-ents
        --converter auto
        --train-size 0.8
        --seed 42
        --shuffle
      - >-
        python -m scripts.convert_to_spans
        assets/anem-test.iob corpus/ner/
        --use-ents
        --converter auto
    deps:
      - assets/anem-train.iob
      - assets/anem-test.iob
    outputs:
      - corpus/ner/anem-train.spacy
      - corpus/ner/anem-train-dev.spacy
      - corpus/ner/anem-test.spacy
      - corpus/spancat/anem-train.spacy
      - corpus/spancat/anem-train-dev.spacy
      - corpus/spancat/anem-test.spacy

  - name: "train-anem"
    help: "Train an NER and Spancat model for AnEM"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/anem/ner/
        --paths.train corpus/ner/anem-train.spacy
        --paths.dev corpus/ner/anem-train-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/anem/spancat/
        --paths.train corpus/spancat/anem-train.spacy
        --paths.dev corpus/spancat/anem-train-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/ner/anem-train.spacy
      - corpus/ner/anem-train-dev.spacy
      - corpus/spancat/anem-train.spacy
      - corpus/spancat/anem-train-dev.spacy
    outputs:
      - training/anem/ner/model-best
      - training/anem/spancat/model-best

  - name: "evaluate-anem"
    help: "Evaluate AnEM dataset"
    script:
      - "mkdir -p metrics/anem/spancat"
      - >-
        python -m spacy evaluate
        training/anem/spancat/model-best
        corpus/spancat/anem-test.spacy
        --output metrics/anem/spancat/scores.json
      - "mkdir -p metrics/anem/ner"
      - >-
        python -m spacy evaluate
        training/anem/ner/model-best
        corpus/ner/anem-test.spacy
        --output metrics/anem/ner/scores.json
    deps:
      - training/anem/ner/model-best
      - training/anem/spancat/model-best
      - corpus/ner/anem-test.spacy
      - corpus/spancat/anem-test.spacy
    outputs:
      - metrics/anem/ner/scores.json
      - metrics/anem/spancat/scores.json

  - name: "convert-wikigold"
    help: "Convert the Wikigold dataset into the spaCy format"
    script:
      - >-
        python -m scripts.convert_to_spans
        assets/wikigold.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - assets/wikigold.iob
    outputs:
      - corpus/spancat/wikigold.spacy

  - name: "clean-wikineural"
    help: "Remove unnecessary indices from wikineural data"
    script:
      - python -m scripts.remove_indices assets/de-wikineural-train.iob
      - python -m scripts.remove_indices assets/de-wikineural-dev.iob
      # - python -m scripts.remove_indices assets/de-wikineural-test.iob
      - python -m scripts.remove_indices assets/en-wikineural-train.iob
      - python -m scripts.remove_indices assets/en-wikineural-dev.iob
      - python -m scripts.remove_indices assets/en-wikineural-test.iob
      - python -m scripts.remove_indices assets/es-wikineural-train.iob
      - python -m scripts.remove_indices assets/es-wikineural-dev.iob
      - python -m scripts.remove_indices assets/es-wikineural-test.iob
      - python -m scripts.remove_indices assets/nl-wikineural-train.iob
      - python -m scripts.remove_indices assets/nl-wikineural-dev.iob
      - python -m scripts.remove_indices assets/nl-wikineural-test.iob
    deps:
      - assets/de-wikineural-train.iob
      - assets/de-wikineural-dev.iob
      - assets/en-wikineural-train.iob
      - assets/en-wikineural-dev.iob
      - assets/en-wikineural-test.iob
      - assets/es-wikineural-train.iob
      - assets/es-wikineural-dev.iob
      - assets/es-wikineural-test.iob
      - assets/nl-wikineural-train.iob
      - assets/nl-wikineural-dev.iob
      - assets/nl-wikineural-test.iob
    outputs:
      - assets/de-wikineural-train.iob
      - assets/de-wikineural-dev.iob
      - assets/en-wikineural-train.iob
      - assets/en-wikineural-dev.iob
      - assets/en-wikineural-test.iob
      - assets/es-wikineural-train.iob
      - assets/es-wikineural-dev.iob
      - assets/es-wikineural-test.iob
      - assets/nl-wikineural-train.iob
      - assets/nl-wikineural-dev.iob
      - assets/nl-wikineural-test.iob

  - name: "convert-wikineural-spans"
    help: "Convert WikiNeural dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # - python -m scripts.remove_indices assets/de-wikineural-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-wikineural-test.iob corpus/spancat/
      #   --spans-key ${vars.spans_key}
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-train.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-dev.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-test.iob corpus/spancat/
        --spans-key ${vars.spans_key}
        --converter auto
    deps:
      - "assets/de-wikineural-train.iob"
      - "assets/de-wikineural-dev.iob"
      # - "assets/de-wikineural-test.iob"
      - "assets/en-wikineural-train.iob"
      - "assets/en-wikineural-dev.iob"
      - "assets/en-wikineural-test.iob"
      - "assets/es-wikineural-train.iob"
      - "assets/es-wikineural-dev.iob"
      - "assets/es-wikineural-test.iob"
      - "assets/nl-wikineural-train.iob"
      - "assets/nl-wikineural-dev.iob"
      - "assets/nl-wikineural-test.iob"
    outputs:
      - "corpus/spancat/de-wikineural-train.spacy"
      - "corpus/spancat/de-wikineural-dev.spacy"
      # - "corpus/spancat/de-wikineural-test.spacy"
      - "corpus/spancat/en-wikineural-train.spacy"
      - "corpus/spancat/en-wikineural-dev.spacy"
      - "corpus/spancat/en-wikineural-test.spacy"
      - "corpus/spancat/es-wikineural-train.spacy"
      - "corpus/spancat/es-wikineural-dev.spacy"
      - "corpus/spancat/es-wikineural-test.spacy"
      - "corpus/spancat/nl-wikineural-train.spacy"
      - "corpus/spancat/nl-wikineural-dev.spacy"
      - "corpus/spancat/nl-wikineural-test.spacy"

  - name: "convert-wikineural-ents"
    help: "Convert WikiNeural dataset (de, en, es, nl) into the spaCy format"
    script:
      # Convert de dataset
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/de-wikineural-dev.iob corpus/ner/
        --use-ents
      # - python -m scripts.remove_indices assets/de-wikineural-test.iob
      # - >-
      #   python -m scripts.convert_to_spans
      #   assets/de-wikineural-test.iob corpus/spancat/
      #   --converter auto
      # Convert en dataset
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/en-wikineural-test.iob corpus/ner/
        --use-ents
      # Convert es dataset
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/es-wikineural-test.iob corpus/ner/
        --use-ents
      # Convert nl dataset
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-train.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-dev.iob corpus/ner/
        --use-ents
      - >-
        python -m scripts.convert_to_spans
        assets/nl-wikineural-test.iob corpus/ner/
        --use-ents
    deps:
      - "assets/de-wikineural-train.iob"
      - "assets/de-wikineural-dev.iob"
      # - "assets/de-wikineural-test.iob"
      - "assets/en-wikineural-train.iob"
      - "assets/en-wikineural-dev.iob"
      - "assets/en-wikineural-test.iob"
      - "assets/es-wikineural-train.iob"
      - "assets/es-wikineural-dev.iob"
      - "assets/es-wikineural-test.iob"
      - "assets/nl-wikineural-train.iob"
      - "assets/nl-wikineural-dev.iob"
      - "assets/nl-wikineural-test.iob"
    outputs:
      - "corpus/ner/de-wikineural-train.spacy"
      - "corpus/ner/de-wikineural-dev.spacy"
      # - "corpus/spancat/de-wikineural-test.spacy"
      - "corpus/ner/en-wikineural-train.spacy"
      - "corpus/ner/en-wikineural-dev.spacy"
      - "corpus/ner/en-wikineural-test.spacy"
      - "corpus/ner/es-wikineural-train.spacy"
      - "corpus/ner/es-wikineural-dev.spacy"
      - "corpus/ner/es-wikineural-test.spacy"
      - "corpus/ner/nl-wikineural-train.spacy"
      - "corpus/ner/nl-wikineural-dev.spacy"
      - "corpus/ner/nl-wikineural-test.spacy"

  - name: "train-wikineural"
    help: "Train an ner and spancat model for Wikineural datasets"
    script:
      # Train de dataset
      - python -m spacy download de_core_news_lg
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/de-wikineural/ner/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/ner/de-wikineural-train.spacy
        --paths.dev corpus/ner/de-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/de-wikineural/spancat/
        --paths.vectors de_core_news_lg
        --nlp.lang de
        --paths.train corpus/spancat/de-wikineural-train.spacy
        --paths.dev corpus/spancat/de-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train en dataset
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/en-wikineural/ner/
        --paths.train corpus/ner/en-wikineural-train.spacy
        --paths.dev corpus/ner/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/en-wikineural/spancat/
        --paths.train corpus/spancat/en-wikineural-train.spacy
        --paths.dev corpus/spancat/en-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train es dataset
      - python -m spacy download es_core_news_lg
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/es-wikineural/ner/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/ner/es-wikineural-train.spacy
        --paths.dev corpus/ner/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/es-wikineural/spancat/
        --paths.vectors es_core_news_lg
        --nlp.lang es
        --paths.train corpus/spancat/es-wikineural-train.spacy
        --paths.dev corpus/spancat/es-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      # Train nl dataset
      - python -m spacy download nl_core_news_lg
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/nl-wikineural/ner/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/ner/nl-wikineural-train.spacy
        --paths.dev corpus/ner/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy train
        configs/spancat.cfg
        --output training/nl-wikineural/spancat/
        --paths.vectors nl_core_news_lg
        --nlp.lang nl
        --paths.train corpus/spancat/nl-wikineural-train.spacy
        --paths.dev corpus/spancat/nl-wikineural-dev.spacy
        --gpu-id ${vars.gpu_id}
    outputs:
      - training/de-wikineural/ner/model-best
      - training/en-wikineural/ner/model-best
      - training/es-wikineural/ner/model-best
      - training/nl-wikineural/ner/model-best
      - training/de-wikineural/spancat/model-best
      - training/en-wikineural/spancat/model-best
      - training/es-wikineural/spancat/model-best
      - training/nl-wikineural/spancat/model-best

  - name: "evaluate-wikineural"
    help: "Evaluate the results for the Wikineural datasets"
    script:
      # FIXME: There seems to be something wrong in the ConLL
      # dataset for the 'de' test set, so I can't evaluate it yet
      # Evaluate en dataset
      - "mkdir -p metrics/en-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/en-wikineural/spancat/model-best
        corpus/spancat/en-wikineural-test.spacy
        --output metrics/en-wikineural/spancat/scores.json
      - "mkdir -p metrics/en-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/en-wikineural/ner/model-best
        corpus/ner/en-wikineural-test.spacy
        --output metrics/en-wikineural/ner/scores.json
      # Evaluate es dataset
      - "mkdir -p metrics/es-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/es-wikineural/spancat/model-best
        corpus/spancat/es-wikineural-test.spacy
        --output metrics/es-wikineural/spancat/scores.json
      - "mkdir -p metrics/es-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/es-wikineural/ner/model-best
        corpus/ner/es-wikineural-test.spacy
        --output metrics/es-wikineural/ner/scores.json
      # Evaluate nl dataset
      - "mkdir -p metrics/nl-wikineural/spancat"
      - >-
        python -m spacy evaluate
        training/nl-wikineural/spancat/model-best
        corpus/spancat/nl-wikineural-test.spacy
        --output metrics/nl-wikineural/spancat/scores.json
      - "mkdir -p metrics/nl-wikineural/ner"
      - >-
        python -m spacy evaluate
        training/nl-wikineural/ner/model-best
        corpus/ner/nl-wikineural-test.spacy
        --output metrics/nl-wikineural/ner/scores.json
    deps:
      - training/en-wikineural/ner/model-best
      - training/es-wikineural/ner/model-best
      - training/nl-wikineural/ner/model-best
      - training/en-wikineural/spancat/model-best
      - training/es-wikineural/spancat/model-best
      - training/nl-wikineural/spancat/model-best
      - corpus/ner/en-wikineural-test.spacy
      - corpus/ner/es-wikineural-test.spacy
      - corpus/ner/nl-wikineural-test.spacy
      - corpus/spancat/en-wikineural-test.spacy
      - corpus/spancat/es-wikineural-test.spacy
      - corpus/spancat/nl-wikineural-test.spacy
    outputs:
      - metrics/en-wikineural/ner/scores.json
      - metrics/es-wikineural/ner/scores.json
      - metrics/nl-wikineural/ner/scores.json
      - metrics/en-wikineural/spancat/scores.json
      - metrics/es-wikineural/spancat/scores.json
      - metrics/nl-wikineural/spancat/scores.json

  - name: "clean"
    help: "Remove intermediary files"
    script:
      - "rm -rf assets/*"
      - "rm -rf corpus/spancat/*"
      - "rm -rf training/*"
      - "rm -rf metrics/*"
